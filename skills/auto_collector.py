"""
è‡ªåŠ¨é‡‡é›† Skill

ä»ç½‘ç»œè‡ªåŠ¨é‡‡é›†é’“é±¼çŸ¥è¯†
æ”¯æŒåçˆ¬åº”å¯¹æœºåˆ¶
"""
import re
import time
import json
import random
import gzip
import io
from typing import Dict, Any, List, Optional, Tuple
from urllib.request import Request, urlopen, build_opener, HTTPCookieProcessor, ProxyHandler
from urllib.error import URLError, HTTPError
from urllib.parse import quote
from http.cookiejar import CookieJar

from llm import LLMFactory, BaseLLM, Message
from config.sources import DATA_SOURCES, get_search_keywords, DataSource
from skills.knowledge_collector import KnowledgeCollector
from skills.knowledge_merger import KnowledgeMerger


USER_AGENTS = [
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

DEFAULT_HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0",
}


class AutoCollector:
    """è‡ªåŠ¨é‡‡é›†å™¨ï¼ˆå¸¦åçˆ¬åº”å¯¹æœºåˆ¶ï¼‰"""
    
    def __init__(
        self, 
        llm: Optional[BaseLLM] = None,
        min_delay: float = 2.0,
        max_delay: float = 5.0,
        max_retries: int = 3,
        proxy: Optional[str] = None,
        debug: bool = False
    ):
        """
        åˆå§‹åŒ–è‡ªåŠ¨é‡‡é›†å™¨
        
        Args:
            llm: LLM å®ä¾‹
            min_delay: æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
            max_delay: æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            proxy: ä»£ç†åœ°å€ï¼Œå¦‚ "http://127.0.0.1:7890"
            debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
        """
        if llm:
            self.llm = llm
        else:
            self.llm = LLMFactory.get_first_available()
        self.collector = KnowledgeCollector(self.llm)
        self.merger = KnowledgeMerger()
        
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.max_retries = max_retries
        self.debug = debug
        
        self.cookie_jar = CookieJar()
        
        self.opener = self._build_opener(proxy)
        
        self.request_count = 0
        self.success_count = 0
        self.fail_count = 0
    
    def _build_opener(self, proxy: Optional[str] = None):
        """æ„å»º URL opener"""
        handlers = [HTTPCookieProcessor(self.cookie_jar)]
        
        if proxy:
            proxy_handler = ProxyHandler({
                "http": proxy,
                "https": proxy
            })
            handlers.append(proxy_handler)
        
        return build_opener(*handlers)
    
    def _random_delay(self):
        """éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»æ“ä½œ"""
        delay = random.uniform(self.min_delay, self.max_delay)
        if self.debug:
            print(f"[DEBUG] å»¶è¿Ÿ {delay:.2f} ç§’...")
        time.sleep(delay)
    
    def _get_random_ua(self) -> str:
        """è·å–éšæœº User-Agent"""
        return random.choice(USER_AGENTS)
    
    def _decompress_content(self, response, content: bytes) -> bytes:
        """è§£å‹ç¼©å†…å®¹"""
        encoding = response.headers.get("Content-Encoding", "")
        if "gzip" in encoding:
            try:
                return gzip.decompress(content)
            except Exception:
                return content
        return content
    
    def fetch_url(
        self, 
        url: str, 
        headers: Optional[Dict[str, str]] = None,
        retry_count: int = 0
    ) -> str:
        """
        è·å–ç½‘é¡µå†…å®¹ï¼ˆå¸¦åçˆ¬æªæ–½ï¼‰
        
        Args:
            url: ç½‘é¡µ URL
            headers: é¢å¤–è¯·æ±‚å¤´
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        Returns:
            ç½‘é¡µæ–‡æœ¬å†…å®¹
        """
        self.request_count += 1
        
        request_headers = DEFAULT_HEADERS.copy()
        request_headers["User-Agent"] = self._get_random_ua()
        
        if headers:
            request_headers.update(headers)
        
        if self.debug:
            print(f"[DEBUG] è¯·æ±‚: {url}")
            print(f"[DEBUG] UA: {request_headers['User-Agent'][:50]}...")
        
        try:
            request = Request(url, headers=request_headers)
            
            with self.opener.open(request, timeout=15) as response:
                content = response.read()
                
                content = self._decompress_content(response, content)
                
                text = content.decode("utf-8", errors="ignore")
                
                self.success_count += 1
                
                if self.debug:
                    print(f"[DEBUG] æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(text)} å­—ç¬¦")
                
                return text
                
        except HTTPError as e:
            self.fail_count += 1
            
            if e.code == 403:
                if retry_count < self.max_retries:
                    wait_time = 2 ** (retry_count + 1)
                    if self.debug:
                        print(f"[DEBUG] 403 é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    return self.fetch_url(url, headers, retry_count + 1)
                return f"è·å–å¤±è´¥: HTTP {e.code} (å·²é‡è¯• {self.max_retries} æ¬¡)"
            
            return f"è·å–å¤±è´¥: HTTP {e.code}"
            
        except URLError as e:
            self.fail_count += 1
            
            if retry_count < self.max_retries:
                wait_time = 2 ** (retry_count + 1)
                if self.debug:
                    print(f"[DEBUG] ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                return self.fetch_url(url, headers, retry_count + 1)
            
            return f"è·å–å¤±è´¥: {e.reason}"
            
        except Exception as e:
            self.fail_count += 1
            return f"è·å–å¤±è´¥: {e}"
    
    def extract_text_from_html(self, html: str) -> str:
        """
        ä» HTML ä¸­æå–çº¯æ–‡æœ¬
        
        Args:
            html: HTML å†…å®¹
            
        Returns:
            çº¯æ–‡æœ¬
        """
        text = html
        
        script_pattern = re.compile(r'<script[^>]*>[\s\S]*?</script>', re.IGNORECASE)
        style_pattern = re.compile(r'<style[^>]*>[\s\S]*?</style>', re.IGNORECASE)
        text = script_pattern.sub('', text)
        text = style_pattern.sub('', text)
        
        text = re.sub(r'<br[^>]*>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</p>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</div>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<[^>]+>', ' ', text)
        
        text = re.sub(r'&nbsp;', ' ', text)
        text = re.sub(r'&amp;', '&', text)
        text = re.sub(r'&lt;', '<', text)
        text = re.sub(r'&gt;', '>', text)
        text = re.sub(r'&quot;', '"', text)
        
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    def extract_content_chunks(self, text: str, min_length: int = 200) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå†…å®¹å—
        
        Args:
            text: é•¿æ–‡æœ¬
            min_length: æœ€å°å—é•¿åº¦
            
        Returns:
            å†…å®¹å—åˆ—è¡¨
        """
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            if len(current_chunk) + len(para) < 2000:
                current_chunk += "\n\n" + para if current_chunk else para
            else:
                if len(current_chunk) >= min_length:
                    chunks.append(current_chunk)
                current_chunk = para
        
        if len(current_chunk) >= min_length:
            chunks.append(current_chunk)
        
        return chunks
    
    def collect_from_url(
        self, 
        url: str, 
        data_type: str,
        auto_save: bool = False
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        ä» URL é‡‡é›†çŸ¥è¯†
        
        Args:
            url: ç½‘é¡µ URL
            data_type: æ•°æ®ç±»å‹
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜
            
        Returns:
            (æå–çš„æ•°æ®åˆ—è¡¨, æ¶ˆæ¯åˆ—è¡¨)
        """
        messages = []
        results = []
        
        self._random_delay()
        
        messages.append(f"æ­£åœ¨è·å–: {url}")
        html = self.fetch_url(url)
        
        if html.startswith("è·å–å¤±è´¥"):
            messages.append(f"  {html}")
            return results, messages
        
        messages.append(f"  è·å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(html)} å­—ç¬¦")
        
        text = self.extract_text_from_html(html)
        messages.append(f"  æå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        chunks = self.extract_content_chunks(text)
        messages.append(f"  åˆ†å‰²ä¸º {len(chunks)} ä¸ªå†…å®¹å—")
        
        for i, chunk in enumerate(chunks):
            messages.append(f"\n  å¤„ç†ç¬¬ {i+1}/{len(chunks)} å—...")
            
            try:
                data = self.collector.collect(chunk, data_type)
                
                if data and data.get("name"):
                    results.append(data)
                    messages.append(f"    âœ“ æå–åˆ°: {data.get('name')}")
                    
                    if auto_save:
                        success, msg = self.merger.merge(data, data_type, strategy="merge")
                        messages.append(f"      {msg}")
                
                time.sleep(0.5)
                
            except Exception as e:
                messages.append(f"    âœ— å¤„ç†å¤±è´¥: {e}")
        
        return results, messages
    
    def collect_from_source(
        self,
        source_name: str,
        data_type: str,
        max_pages: int = 3,
        auto_save: bool = False
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        ä»æ•°æ®æºé‡‡é›†çŸ¥è¯†
        
        Args:
            source_name: æ•°æ®æºåç§°
            data_type: æ•°æ®ç±»å‹
            max_pages: æœ€å¤§é¡µé¢æ•°
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜
            
        Returns:
            (æå–çš„æ•°æ®åˆ—è¡¨, æ¶ˆæ¯åˆ—è¡¨)
        """
        messages = []
        results = []
        
        source = DATA_SOURCES.get(source_name)
        if not source:
            messages.append(f"æœªçŸ¥æ•°æ®æº: {source_name}")
            return results, messages
        
        keywords = get_search_keywords(data_type)
        if not keywords:
            messages.append(f"æœªå®šä¹‰ {data_type} çš„æœç´¢å…³é”®è¯")
            return results, messages
        
        messages.append(f"ä» {source.name} é‡‡é›† {data_type} çŸ¥è¯†")
        messages.append(f"æœç´¢å…³é”®è¯: {keywords[:3]}...")
        
        for keyword in keywords[:max_pages]:
            search_url = source.get_search_url(keyword)
            if not search_url:
                continue
            
            messages.append(f"\næœç´¢: {keyword}")
            messages.append(f"URL: {search_url}")
            
            self._random_delay()
            html = self.fetch_url(search_url, source.headers)
            
            if html.startswith("è·å–å¤±è´¥"):
                messages.append(f"  {html}")
                continue
            
            links = self.extract_links(html, source.base_url)
            messages.append(f"  æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
            
            for j, link in enumerate(links[:2]):
                messages.append(f"\n  è®¿é—®: {link}")
                page_results, page_messages = self.collect_from_url(link, data_type, auto_save)
                results.extend(page_results)
                messages.extend(page_messages)
        
        messages.append(f"\né‡‡é›†å®Œæˆï¼Œå…±æå– {len(results)} æ¡æ•°æ®")
        messages.append(f"ç»Ÿè®¡: è¯·æ±‚ {self.request_count} æ¬¡ï¼ŒæˆåŠŸ {self.success_count} æ¬¡ï¼Œå¤±è´¥ {self.fail_count} æ¬¡")
        return results, messages
    
    def extract_links(self, html: str, base_url: str) -> List[str]:
        """
        ä» HTML ä¸­æå–é“¾æ¥
        
        Args:
            html: HTML å†…å®¹
            base_url: åŸºç¡€ URL
            
        Returns:
            é“¾æ¥åˆ—è¡¨
        """
        links = []
        
        link_pattern = re.compile(r'href=["\']([^"\']+)["\']', re.IGNORECASE)
        
        for match in link_pattern.finditer(html):
            href = match.group(1)
            
            if href.startswith("//"):
                href = "https:" + href
            elif href.startswith("/"):
                href = base_url.rstrip("/") + href
            elif not href.startswith("http"):
                continue
            
            if any(x in href.lower() for x in ["javascript:", "#", ".jpg", ".png", ".gif", ".css", ".js"]):
                continue
            
            if href not in links:
                links.append(href)
        
        return links[:10]
    
    def quick_collect(
        self,
        keyword: str,
        data_type: str,
        auto_save: bool = False
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        å¿«é€Ÿé‡‡é›†ï¼ˆä½¿ç”¨æœç´¢å¼•æ“é£æ ¼çš„å…³é”®è¯æœç´¢ï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            data_type: æ•°æ®ç±»å‹
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜
            
        Returns:
            (æå–çš„æ•°æ®åˆ—è¡¨, æ¶ˆæ¯åˆ—è¡¨)
        """
        messages = []
        results = []
        
        messages.append(f"å¿«é€Ÿé‡‡é›†: {keyword}")
        
        for source_name, source in DATA_SOURCES.items():
            search_url = source.get_search_url(keyword)
            if not search_url:
                continue
            
            messages.append(f"\nå°è¯• {source.name}...")
            
            self._random_delay()
            html = self.fetch_url(search_url, source.headers)
            
            if html.startswith("è·å–å¤±è´¥"):
                messages.append(f"  {html}")
                continue
            
            links = self.extract_links(html, source.base_url)
            messages.append(f"  æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
            
            for link in links[:2]:
                try:
                    page_results, page_messages = self.collect_from_url(link, data_type, auto_save)
                    if page_results:
                        results.extend(page_results)
                        messages.extend(page_messages)
                        break
                except Exception as e:
                    messages.append(f"  é”™è¯¯: {e}")
            
            if results:
                break
        
        messages.append(f"\nç»Ÿè®¡: è¯·æ±‚ {self.request_count} æ¬¡ï¼ŒæˆåŠŸ {self.success_count} æ¬¡ï¼Œå¤±è´¥ {self.fail_count} æ¬¡")
        return results, messages
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–è¯·æ±‚ç»Ÿè®¡"""
        return {
            "total": self.request_count,
            "success": self.success_count,
            "failed": self.fail_count
        }


def format_collect_results(results: List[Dict[str, Any]], messages: List[str]) -> str:
    """æ ¼å¼åŒ–é‡‡é›†ç»“æœ"""
    lines = ["\nğŸ“¡ è‡ªåŠ¨é‡‡é›†ç»“æœ", "â•" * 50]
    
    for msg in messages:
        lines.append(msg)
    
    lines.append("â”€" * 50)
    lines.append(f"å…±æå– {len(results)} æ¡æ•°æ®:")
    
    for i, data in enumerate(results, 1):
        lines.append(f"  {i}. {data.get('name', 'æœªçŸ¥')}")
    
    lines.append("â•" * 50)
    
    return "\n".join(lines)
