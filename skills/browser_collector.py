"""
æµè§ˆå™¨é‡‡é›† Skill

ä½¿ç”¨ Playwright æ¨¡æ‹Ÿæµè§ˆå™¨è¿›è¡ŒçŸ¥è¯†é‡‡é›†
æ”¯æŒåçˆ¬åº”å¯¹ï¼šæ¨¡æ‹Ÿäººç±»æ“ä½œã€éšæœºå»¶è¿Ÿã€æ— å¤´æ¨¡å¼
"""
import re
import os
import time
import random
import platform
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

from llm import LLMFactory, BaseLLM
from config.sources import DATA_SOURCES, get_search_keywords, DataSource
from skills.knowledge_collector import KnowledgeCollector
from skills.knowledge_merger import KnowledgeMerger


PLAYWRIGHT_AVAILABLE = False
BROWSER_TYPE = None

try:
    from playwright.sync_api import sync_playwright, Browser, Page, BrowserContext
    PLAYWRIGHT_AVAILABLE = True
    BROWSER_TYPE = "playwright"
except ImportError:
    pass


def find_chrome_executable() -> Optional[str]:
    """æŸ¥æ‰¾ç³»ç»Ÿ Chrome å¯æ‰§è¡Œæ–‡ä»¶"""
    system = platform.system()
    
    if system == "Darwin":
        paths = [
            "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
            "/Applications/Chromium.app/Contents/MacOS/Chromium",
        ]
    elif system == "Windows":
        paths = [
            r"C:\Program Files\Google\Chrome\Application\chrome.exe",
            r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
            os.path.expandvars(r"%LOCALAPPDATA%\Google\Chrome\Application\chrome.exe"),
        ]
    else:
        paths = [
            "/usr/bin/google-chrome",
            "/usr/bin/chromium-browser",
            "/usr/bin/chromium",
        ]
    
    for path in paths:
        if Path(path).exists():
            return path
    
    return None


class BrowserCollector:
    """æµè§ˆå™¨é‡‡é›†å™¨ï¼ˆä½¿ç”¨ Playwrightï¼‰"""
    
    def __init__(
        self,
        llm: Optional[BaseLLM] = None,
        headless: bool = True,
        slow_mo: int = 100,
        timeout: int = 30000,
        debug: bool = False
    ):
        """
        åˆå§‹åŒ–æµè§ˆå™¨é‡‡é›†å™¨
        
        Args:
            llm: LLM å®ä¾‹
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆTrue=åå°è¿è¡Œï¼ŒFalse=æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰
            slow_mo: æ“ä½œå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ï¼Œæ¨¡æ‹Ÿäººç±»æ“ä½œé€Ÿåº¦
            timeout: é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰
            debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
        """
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError(
                "Playwright æœªå®‰è£…ï¼\n"
                "è¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
                "  pip install playwright -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
                "æˆ–è€…ä½¿ç”¨ /collect å‘½ä»¤æ‰‹åŠ¨ç²˜è´´å†…å®¹"
            )
        
        if llm:
            self.llm = llm
        else:
            self.llm = LLMFactory.get_first_available()
        self.collector = KnowledgeCollector(self.llm)
        self.merger = KnowledgeMerger()
        
        self.headless = headless
        self.slow_mo = slow_mo
        self.timeout = timeout
        self.debug = debug
        
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        
        self.request_count = 0
        self.success_count = 0
        self.fail_count = 0
    
    def _init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        if self.browser:
            return
        
        if self.debug:
            print("[DEBUG] å¯åŠ¨æµè§ˆå™¨...")
        
        self.playwright = sync_playwright().start()
        
        chrome_path = find_chrome_executable()
        
        launch_options = {
            "headless": self.headless,
            "slow_mo": self.slow_mo,
            "args": [
                "--disable-blink-features=AutomationControlled",
                "--disable-infobars",
                "--no-first-run",
                "--no-default-browser-check",
            ]
        }
        
        if chrome_path:
            if self.debug:
                print(f"[DEBUG] ä½¿ç”¨ç³»ç»Ÿ Chrome: {chrome_path}")
            launch_options["executable_path"] = chrome_path
            launch_options["channel"] = None
        else:
            if self.debug:
                print("[DEBUG] ä½¿ç”¨ Playwright å†…ç½® Chromium")
        
        try:
            self.browser = self.playwright.chromium.launch(**launch_options)
        except Exception as e:
            if "doesn't exist" in str(e) or "not installed" in str(e):
                raise ImportError(
                    "æµè§ˆå™¨æœªå®‰è£…ï¼\n"
                    "è¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… Chromiumï¼š\n"
                    "  python3 -m playwright install chromium\n"
                    "æˆ–è€…å®‰è£… Google Chrome æµè§ˆå™¨\n"
                    "æˆ–è€…ä½¿ç”¨ /collect å‘½ä»¤æ‰‹åŠ¨ç²˜è´´å†…å®¹"
                )
            raise
        
        self.context = self.browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            locale="zh-CN",
            ignore_https_errors=True,
        )
        
        self.context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
        
        if self.debug:
            print("[DEBUG] æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
    
    def _close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.context:
            self.context.close()
            self.context = None
        if self.browser:
            self.browser.close()
            self.browser = None
        if self.playwright:
            self.playwright.stop()
            self.playwright = None
        
        if self.debug:
            print("[DEBUG] æµè§ˆå™¨å·²å…³é—­")
    
    def _random_delay(self, min_sec: float = 1.0, max_sec: float = 3.0):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(min_sec, max_sec)
        if self.debug:
            print(f"[DEBUG] å»¶è¿Ÿ {delay:.2f} ç§’...")
        time.sleep(delay)
    
    def _human_like_scroll(self, page):
        """æ¨¡æ‹Ÿäººç±»æ»šåŠ¨"""
        scroll_times = random.randint(2, 5)
        for _ in range(scroll_times):
            scroll_distance = random.randint(200, 500)
            page.mouse.wheel(0, scroll_distance)
            time.sleep(random.uniform(0.3, 0.8))
    
    def _human_like_mouse_move(self, page, x: int, y: int):
        """æ¨¡æ‹Ÿäººç±»é¼ æ ‡ç§»åŠ¨"""
        current = page.evaluate("() => ({ x: window.mouseX || 0, y: window.mouseY || 0 })")
        steps = random.randint(10, 20)
        
        for i in range(steps):
            progress = (i + 1) / steps
            intermediate_x = int(current["x"] + (x - current["x"]) * progress)
            intermediate_y = int(current["y"] + (y - current["y"]) * progress)
            page.mouse.move(intermediate_x, intermediate_y)
            time.sleep(random.uniform(0.01, 0.03))
        
        page.mouse.move(x, y)
    
    def fetch_page(self, url: str, wait_selector: str = None) -> str:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: é¡µé¢ URL
            wait_selector: ç­‰å¾…çš„å…ƒç´ é€‰æ‹©å™¨
            
        Returns:
            é¡µé¢æ–‡æœ¬å†…å®¹
        """
        self._init_browser()
        self.request_count += 1
        
        page = self.context.new_page()
        
        try:
            if self.debug:
                print(f"[DEBUG] è®¿é—®: {url}")
            
            page.goto(url, timeout=self.timeout, wait_until="networkidle")
            
            self._random_delay(0.5, 1.5)
            
            if wait_selector:
                page.wait_for_selector(wait_selector, timeout=self.timeout)
            
            self._human_like_scroll(page)
            
            content = page.content()
            text = self._extract_text(content)
            
            self.success_count += 1
            
            if self.debug:
                print(f"[DEBUG] è·å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(text)} å­—ç¬¦")
            
            return text
            
        except Exception as e:
            self.fail_count += 1
            if self.debug:
                print(f"[DEBUG] è·å–å¤±è´¥: {e}")
            return f"è·å–å¤±è´¥: {e}"
            
        finally:
            page.close()
    
    def _extract_text(self, html: str) -> str:
        """ä» HTML ä¸­æå–çº¯æ–‡æœ¬"""
        text = html
        
        script_pattern = re.compile(r'<script[^>]*>[\s\S]*?</script>', re.IGNORECASE)
        style_pattern = re.compile(r'<style[^>]*>[\s\S]*?</style>', re.IGNORECASE)
        text = script_pattern.sub('', text)
        text = style_pattern.sub('', text)
        
        text = re.sub(r'<br[^>]*>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</p>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</div>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<[^>]+>', ' ', text)
        
        text = re.sub(r'&nbsp;', ' ', text)
        text = re.sub(r'&amp;', '&', text)
        text = re.sub(r'&lt;', '<', text)
        text = re.sub(r'&gt;', '>', text)
        text = re.sub(r'&quot;', '"', text)
        
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    def search_and_collect(
        self,
        source_name: str,
        data_type: str,
        max_pages: int = 3,
        auto_save: bool = False
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        æœç´¢å¹¶é‡‡é›†çŸ¥è¯†
        
        Args:
            source_name: æ•°æ®æºåç§°
            data_type: æ•°æ®ç±»å‹
            max_pages: æœ€å¤§é¡µé¢æ•°
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜
            
        Returns:
            (æå–çš„æ•°æ®åˆ—è¡¨, æ¶ˆæ¯åˆ—è¡¨)
        """
        messages = []
        results = []
        
        source = DATA_SOURCES.get(source_name)
        if not source:
            messages.append(f"æœªçŸ¥æ•°æ®æº: {source_name}")
            return results, messages
        
        keywords = get_search_keywords(data_type)
        if not keywords:
            messages.append(f"æœªå®šä¹‰ {data_type} çš„æœç´¢å…³é”®è¯")
            return results, messages
        
        messages.append(f"ä» {source.name} é‡‡é›† {data_type} çŸ¥è¯†ï¼ˆæµè§ˆå™¨æ¨¡å¼ï¼‰")
        messages.append(f"æœç´¢å…³é”®è¯: {keywords[:3]}...")
        
        try:
            self._init_browser()
            
            for keyword in keywords[:max_pages]:
                search_url = source.get_search_url(keyword)
                if not search_url:
                    continue
                
                messages.append(f"\næœç´¢: {keyword}")
                messages.append(f"URL: {search_url}")
                
                page = self.context.new_page()
                
                try:
                    page.goto(search_url, timeout=self.timeout, wait_until="networkidle")
                    self._random_delay(1.0, 2.0)
                    
                    self._human_like_scroll(page)
                    
                    links = self._extract_links(page, source.base_url)
                    messages.append(f"  æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                    
                    for link in links[:2]:
                        messages.append(f"\n  è®¿é—®: {link}")
                        
                        page_result = self._collect_from_page(page, link, data_type, auto_save)
                        results.extend(page_result["data"])
                        messages.extend([f"    {m}" for m in page_result["messages"]])
                        
                        self._random_delay(2.0, 4.0)
                        
                except Exception as e:
                    messages.append(f"  é¡µé¢å¤„ç†å¤±è´¥: {e}")
                    
                finally:
                    page.close()
                    
        except Exception as e:
            messages.append(f"æµè§ˆå™¨é”™è¯¯: {e}")
            
        finally:
            self._close_browser()
        
        messages.append(f"\né‡‡é›†å®Œæˆï¼Œå…±æå– {len(results)} æ¡æ•°æ®")
        messages.append(f"ç»Ÿè®¡: è¯·æ±‚ {self.request_count} æ¬¡ï¼ŒæˆåŠŸ {self.success_count} æ¬¡ï¼Œå¤±è´¥ {self.fail_count} æ¬¡")
        return results, messages
    
    def _extract_links(self, page, base_url: str) -> List[str]:
        """ä»é¡µé¢ä¸­æå–é“¾æ¥"""
        links = []
        
        try:
            elements = page.query_selector_all("a[href]")
            
            base_domain = base_url.replace("https://", "").replace("http://", "").split("/")[0]
            
            article_patterns = ["/jiqiao/", "/yuhuo/", "/article/", "/post/", "/p/", "/detail/"]
            
            for elem in elements[:100]:
                href = elem.get_attribute("href")
                if not href:
                    continue
                
                if href.startswith("//"):
                    href = "https:" + href
                elif href.startswith("/"):
                    href = base_url.rstrip("/") + href
                elif not href.startswith("http"):
                    continue
                
                if any(x in href.lower() for x in ["javascript:", "#", ".jpg", ".png", ".gif", ".css", ".js", ".pdf"]):
                    continue
                
                if any(x in href.lower() for x in ["baidu.com", "google.com", "bing.com"]):
                    continue
                
                if base_domain not in href:
                    continue
                
                skip_patterns = ["/member/", "/login", "/register", "/about/", "/citys", "/search"]
                if any(x in href.lower() for x in skip_patterns):
                    continue
                
                if href == base_url or href == base_url.rstrip("/"):
                    continue
                
                if href not in links:
                    links.append(href)
                    
        except Exception as e:
            if self.debug:
                print(f"[DEBUG] æå–é“¾æ¥å¤±è´¥: {e}")
        
        return links[:10]
    
    def _collect_from_page(
        self,
        parent_page,
        url: str,
        data_type: str,
        auto_save: bool
    ) -> Dict[str, Any]:
        """ä»å•ä¸ªé¡µé¢é‡‡é›†çŸ¥è¯†"""
        result = {"data": [], "messages": []}
        
        page = self.context.new_page()
        
        try:
            page.goto(url, timeout=self.timeout, wait_until="networkidle")
            self._random_delay(0.5, 1.5)
            
            self._human_like_scroll(page)
            
            content = page.content()
            text = self._extract_text(content)
            
            result["messages"].append(f"è·å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(text)} å­—ç¬¦")
            
            chunks = self._split_text(text)
            result["messages"].append(f"åˆ†å‰²ä¸º {len(chunks)} ä¸ªå†…å®¹å—")
            
            for i, chunk in enumerate(chunks):
                result["messages"].append(f"å¤„ç†ç¬¬ {i+1}/{len(chunks)} å—...")
                
                try:
                    data = self.collector.collect(chunk, data_type)
                    
                    if data and data.get("name"):
                        result["data"].append(data)
                        result["messages"].append(f"âœ“ æå–åˆ°: {data.get('name')}")
                        
                        if auto_save:
                            success, msg = self.merger.merge(data, data_type, strategy="merge")
                            result["messages"].append(f"  {msg}")
                    
                    time.sleep(0.5)
                    
                except Exception as e:
                    result["messages"].append(f"âœ— å¤„ç†å¤±è´¥: {e}")
            
            self.success_count += 1
                    
        except Exception as e:
            self.fail_count += 1
            result["messages"].append(f"è·å–å¤±è´¥: {e}")
            
        finally:
            page.close()
        
        return result
    
    def _split_text(self, text: str, min_length: int = 200) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬"""
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            if len(current_chunk) + len(para) < 2000:
                current_chunk += "\n\n" + para if current_chunk else para
            else:
                if len(current_chunk) >= min_length:
                    chunks.append(current_chunk)
                current_chunk = para
        
        if len(current_chunk) >= min_length:
            chunks.append(current_chunk)
        
        return chunks
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–è¯·æ±‚ç»Ÿè®¡"""
        return {
            "total": self.request_count,
            "success": self.success_count,
            "failed": self.fail_count
        }


def check_playwright_available() -> Tuple[bool, str]:
    """æ£€æŸ¥ Playwright æ˜¯å¦å¯ç”¨"""
    if PLAYWRIGHT_AVAILABLE:
        return True, "Playwright å·²å®‰è£…"
    else:
        return False, (
            "Playwright æœªå®‰è£…ï¼\n"
            "è¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
            "  pip install playwright\n"
            "  playwright install chromium\n"
            "æˆ–è€…ä½¿ç”¨ /collect å‘½ä»¤æ‰‹åŠ¨ç²˜è´´å†…å®¹"
        )


def format_browser_results(results: List[Dict[str, Any]], messages: List[str]) -> str:
    """æ ¼å¼åŒ–æµè§ˆå™¨é‡‡é›†ç»“æœ"""
    lines = ["\nğŸŒ æµè§ˆå™¨é‡‡é›†ç»“æœ", "â•" * 50]
    
    for msg in messages:
        lines.append(msg)
    
    lines.append("â”€" * 50)
    lines.append(f"å…±æå– {len(results)} æ¡æ•°æ®:")
    
    for i, data in enumerate(results, 1):
        lines.append(f"  {i}. {data.get('name', 'æœªçŸ¥')}")
    
    lines.append("â•" * 50)
    
    return "\n".join(lines)
